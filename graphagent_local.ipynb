{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Setup successful.\n"
     ]
    }
   ],
   "source": [
    "# --- Local OpenAI-compatible client (LM Studio / Ollama) ---\n",
    "# Endpoint: http://127.0.0.1:1234  (your server)\n",
    "# Model:    qwen/qwen2.5-vl-7b\n",
    "\n",
    "%pip install -q openai\n",
    "\n",
    "import os\n",
    "from openai import OpenAI\n",
    "\n",
    "BASE_URL = \"http://127.0.0.1:1234/v1\"      # <â€” your server\n",
    "API_KEY  = \"sk-local\"                      # <â€” dummy key, any string works\n",
    "MODEL = \"qwen/qwen2.5-vl-7b\"               # <â€” from your models list\n",
    "\n",
    "client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "def local_chat(prompt: str, *, system: str | None = None,\n",
    "               temperature: float = 0.2, max_tokens: int = 512,\n",
    "               model: str = \"qwen/qwen2.5-vl-7b\") -> str:\n",
    "    msgs = []\n",
    "    if system:\n",
    "        msgs.append({\"role\": \"system\", \"content\": system})\n",
    "    msgs.append({\"role\": \"user\", \"content\": prompt})\n",
    "    resp = client.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=msgs,\n",
    "        temperature=temperature,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "    raw = resp.choices[0].message.content.strip()\n",
    "    return clean_thinking_output(raw)\n",
    "\n",
    "\n",
    "def clean_thinking_output(text: str) -> str:\n",
    "    # If the model outputs <think>â€¦</think>, strip that part\n",
    "    if \"</think>\" in text:\n",
    "        return text.split(\"</think>\")[-1].strip()\n",
    "    if \"<think>\" in text:\n",
    "        return text.split(\"<think>\")[-1].strip()\n",
    "    return text.strip()    \n",
    "    \n",
    "\n",
    "# quick smoketest:\n",
    "print(local_chat(\"Say exactly: setup successful.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-X3WmICe5myt"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gmoores\\AI-Tutorial-Codes-Included\\venv\\Scripts\\python.exe\n",
      "C:\\Users\\gmoores\\AI-Tutorial-Codes-Included\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "print(sys.executable)\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-X3WmICe5myt"
   },
   "outputs": [],
   "source": [
    "# Block 3 (local model setup; no Gemini)\n",
    "import os, json, time, ast, math\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Dict, List, Callable, Any\n",
    "\n",
    "# If networkx isn't installed, the graph still runs without drawing\n",
    "try:\n",
    "    import networkx as nx\n",
    "except ImportError:\n",
    "    nx = None\n",
    "\n",
    "# Reuse local_chat if you already defined it earlier.\n",
    "# If not, this fallback defines it here (expects your local server at 127.0.0.1:1234).\n",
    "try:\n",
    "    local_chat\n",
    "except NameError:\n",
    "    from openai import OpenAI\n",
    "    BASE_URL = os.getenv(\"LLM_ENDPOINT\", \"http://127.0.0.1:1234/v1\")\n",
    "    API_KEY  = os.getenv(\"LLM_API_KEY\", \"sk-local\")\n",
    "    DEFAULT_MODEL = os.getenv(\"LLM_MODEL\", \"qwen/qwen2.5-vl-7b\")\n",
    "    client = OpenAI(base_url=BASE_URL, api_key=API_KEY)\n",
    "\n",
    "    def local_chat(prompt: str, *, system: str | None = None,\n",
    "                   temperature: float = 0.2, max_tokens: int = 512,\n",
    "                   model: str = DEFAULT_MODEL) -> str:\n",
    "        msgs = []\n",
    "        if system:\n",
    "            msgs.append({\"role\": \"system\", \"content\": system})\n",
    "        msgs.append({\"role\": \"user\", \"content\": prompt})\n",
    "        resp = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=msgs,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "        )\n",
    "        return resp.choices[0].message.content.strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "4PgAdOT05q13"
   },
   "outputs": [],
   "source": [
    "# Block 4 (model factory + LLM call utility)\n",
    "def make_model(model_name: str = \"qwen/qwen2.5-vl-7b\"):\n",
    "    system_text = (\n",
    "        \"You are GraphAgent, a principled planner-executor. \"\n",
    "        \"Prefer structured, concise outputs; use provided tools when asked.\"\n",
    "    )\n",
    "    # Return a callable that wraps our local_chat with a fixed system prompt & model\n",
    "    def _call(prompt: str, temperature: float = 0.2):\n",
    "        return local_chat(prompt, system=system_text, temperature=temperature, model=model_name)\n",
    "    return _call\n",
    "\n",
    "def call_llm(model, prompt: str, temperature=0.2) -> str:\n",
    "    # model is the callable returned by make_model(...)\n",
    "    return (model(prompt, temperature=temperature) or \"\").strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0jrDj0nz5tDO"
   },
   "outputs": [],
   "source": [
    "# Block 5 (math + toy RAG helpers)\n",
    "def safe_eval_math(expr: str) -> str:\n",
    "    node = ast.parse(expr, mode=\"eval\")\n",
    "    allowed = (ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Constant,\n",
    "               ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow, ast.Mod,\n",
    "               ast.USub, ast.UAdd, ast.FloorDiv, ast.AST)\n",
    "    def check(n):\n",
    "        if not isinstance(n, allowed):\n",
    "            raise ValueError(\"Unsafe expression\")\n",
    "        for c in ast.iter_child_nodes(n):\n",
    "            check(c)\n",
    "    check(node)\n",
    "    return str(eval(compile(node, \"<math>\", \"eval\"), {\"__builtins__\": {}}, {}))\n",
    "\n",
    "DOCS = [\n",
    "    \"Solar panels convert sunlight to electricity; capacity factor ~20%.\",\n",
    "    \"Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\",\n",
    "    \"RAG = retrieval-augmented generation joins search with prompting.\",\n",
    "    \"LangGraph enables cyclic graphs of agents; good for tool orchestration.\",\n",
    "]\n",
    "def search_docs(q: str, k: int = 3) -> List[str]:\n",
    "    ql = q.lower()\n",
    "    scored = sorted(DOCS, key=lambda d: -sum(w in d.lower() for w in ql.split()))\n",
    "    return scored[:k]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "eBYrj_Eb5uI_"
   },
   "outputs": [],
   "source": [
    "# Block 6 (state + nodes + runner)\n",
    "@dataclass\n",
    "class State:\n",
    "    task: str\n",
    "    plan: str = \"\"\n",
    "    scratch: List[str] = field(default_factory=list)\n",
    "    evidence: List[str] = field(default_factory=list)\n",
    "    result: str = \"\"\n",
    "    step: int = 0\n",
    "    done: bool = False\n",
    "\n",
    "def node_plan(state: State, model) -> str:\n",
    "    prompt = f\"\"\"Plan step-by-step to solve the user task.\n",
    "Task: {state.task}\n",
    "Return JSON: {{\"subtasks\": [\"...\"], \"tools\": {{\"search\": true/false, \"math\": true/false}}, \"success_criteria\": [\"...\"]}}\"\"\"\n",
    "    js = call_llm(model, prompt)\n",
    "    try:\n",
    "        plan = json.loads(js[js.find(\"{\"): js.rfind(\"}\")+1])\n",
    "    except Exception:\n",
    "        plan = {\"subtasks\": [\"Research\", \"Synthesize\"], \"tools\": {\"search\": True, \"math\": False}, \"success_criteria\": [\"clear answer\"]}\n",
    "    state.plan = json.dumps(plan, indent=2)\n",
    "    state.scratch.append(\"PLAN:\\n\"+state.plan)\n",
    "    return \"route\"\n",
    "\n",
    "def node_route(state: State, model) -> str:\n",
    "    prompt = f\"\"\"You are a router. Decide next node.\n",
    "Context scratch:\\n{chr(10).join(state.scratch[-5:])}\n",
    "If math needed -> 'math', if research needed -> 'research', if ready -> 'write'.\n",
    "Return one token from [research, math, write]. Task: {state.task}\"\"\"\n",
    "    choice = call_llm(model, prompt).lower()\n",
    "    if \"math\" in choice and any(ch.isdigit() for ch in state.task):\n",
    "        return \"math\"\n",
    "    if \"research\" in choice or not state.evidence:\n",
    "        return \"research\"\n",
    "    return \"write\"\n",
    "\n",
    "def node_research(state: State, model) -> str:\n",
    "    prompt = f\"\"\"Generate 3 focused search queries for:\n",
    "Task: {state.task}\n",
    "Return as JSON list of strings.\"\"\"\n",
    "    qjson = call_llm(model, prompt)\n",
    "    try:\n",
    "        queries = json.loads(qjson[qjson.find(\"[\"): qjson.rfind(\"]\")+1])[:3]\n",
    "    except Exception:\n",
    "        queries = [state.task, \"background \"+state.task, \"pros cons \"+state.task]\n",
    "    hits = []\n",
    "    for q in queries:\n",
    "        hits.extend(search_docs(q, k=2))\n",
    "    # de-duplicate while preserving order\n",
    "    state.evidence.extend(list(dict.fromkeys(hits)))\n",
    "    state.scratch.append(\"EVIDENCE:\\n- \" + \"\\n- \".join(hits))\n",
    "    return \"route\"\n",
    "\n",
    "def node_math(state: State, model) -> str:\n",
    "    prompt = \"Extract a single arithmetic expression from this task:\\n\"+state.task\n",
    "    expr = call_llm(model, prompt)\n",
    "    expr = \"\".join(ch for ch in expr if ch in \"0123456789+-*/().%^ \")\n",
    "    try:\n",
    "        val = safe_eval_math(expr)\n",
    "        state.scratch.append(f\"MATH: {expr} = {val}\")\n",
    "    except Exception as e:\n",
    "        state.scratch.append(f\"MATH-ERROR: {expr} ({e})\")\n",
    "    return \"route\"\n",
    "\n",
    "def node_write(state: State, model) -> str:\n",
    "    prompt = f\"\"\"Write the final answer.\n",
    "Task: {state.task}\n",
    "Use the evidence and any math results below, cite inline like [1],[2].\n",
    "Evidence:\\n{chr(10).join(f'[{i+1}] '+e for i,e in enumerate(state.evidence))}\n",
    "Notes:\\n{chr(10).join(state.scratch[-5:])}\n",
    "Return a concise, structured answer.\"\"\"\n",
    "    draft = call_llm(model, prompt, temperature=0.3)\n",
    "    state.result = draft\n",
    "    state.scratch.append(\"DRAFT:\\n\"+draft)\n",
    "    return \"critic\"\n",
    "\n",
    "def node_critic(state: State, model) -> str:\n",
    "    prompt = f\"\"\"Critique and improve the answer for factuality, missing steps, and clarity.\n",
    "If fix needed, return improved answer. Else return 'OK'.\n",
    "Answer:\\n{state.result}\\nCriteria:\\n{state.plan}\"\"\"\n",
    "    crit = call_llm(model, prompt)\n",
    "    if crit.strip().upper() != \"OK\" and len(crit) > 30:\n",
    "        state.result = crit.strip()\n",
    "        state.scratch.append(\"REVISED\")\n",
    "    state.done = True\n",
    "    return \"end\"\n",
    "\n",
    "NODES: Dict[str, Callable[[State, Any], str]] = {\n",
    "    \"plan\": node_plan, \"route\": node_route, \"research\": node_research,\n",
    "    \"math\": node_math, \"write\": node_write, \"critic\": node_critic\n",
    "}\n",
    "\n",
    "def run_graph(task: str, model=None, model_name: str = \"qwen/qwen2.5-vl-7b\") -> State:\n",
    "    model_callable = model or make_model(model_name)\n",
    "    state = State(task=task)\n",
    "    cur = \"plan\"\n",
    "    max_steps = 12\n",
    "    while not state.done and state.step < max_steps:\n",
    "        state.step += 1\n",
    "        nxt = NODES[cur](state, model_callable)\n",
    "        if nxt == \"end\":\n",
    "            break\n",
    "        cur = nxt\n",
    "    return state\n",
    "\n",
    "def ascii_graph():\n",
    "    return \"\"\"\n",
    "START -> plan -> route -> (research <-> route) & (math <-> route) -> write -> critic -> END\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 882
    },
    "id": "_ID2dD6Q5HHn",
    "outputId": "51ea467e-b8c5-4aa7-8eef-ba9373a8c2db"
   },
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "ðŸ“ Enter your task:  ðŸ“ Enter your task:  Compare drought-tolerant gardening vs traditional gardening regarding each approaches aesthetics, maintenance requirements, and amount of inputs (fertilizer, water, etc.) for DIY home landscapers looking to redesign their Colorado front yards ; compute 5*7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GRAPH === \n",
      "START -> plan -> route -> (research <-> route) & (math <-> route) -> write -> critic -> END\n",
      "\n",
      "\n",
      "âœ… Result in 10.30s:\n",
      "\n",
      "\n",
      "---- Evidence ----\n",
      "Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "RAG = retrieval-augmented generation joins search with prompting.\n",
      "LangGraph enables cyclic graphs of agents; good for tool orchestration.\n",
      "Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "RAG = retrieval-augmented generation joins search with prompting.\n",
      "Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "RAG = retrieval-augmented generation joins search with prompting.\n",
      "Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "RAG = retrieval-augmented generation joins search with prompting.\n",
      "Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "RAG = retrieval-augmented generation joins search with prompting.\n",
      "Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "LangGraph enables cyclic graphs of agents; good for tool orchestration.\n",
      "\n",
      "---- Scratch (last 5) ----\n",
      "EVIDENCE:\n",
      "- Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- RAG = retrieval-augmented generation joins search with prompting.\n",
      "- LangGraph enables cyclic graphs of agents; good for tool orchestration.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "EVIDENCE:\n",
      "- Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- RAG = retrieval-augmented generation joins search with prompting.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- RAG = retrieval-augmented generation joins search with prompting.\n",
      "EVIDENCE:\n",
      "- Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- RAG = retrieval-augmented generation joins search with prompting.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- RAG = retrieval-augmented generation joins search with prompting.\n",
      "EVIDENCE:\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- RAG = retrieval-augmented generation joins search with prompting.\n",
      "- Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- RAG = retrieval-augmented generation joins search with prompting.\n",
      "EVIDENCE:\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- RAG = retrieval-augmented generation joins search with prompting.\n",
      "- Solar panels convert sunlight to electricity; capacity factor ~20%.\n",
      "- Wind turbines harvest kinetic energy; onshore capacity factor ~35%.\n",
      "- LangGraph enables cyclic graphs of agents; good for tool orchestration.\n",
      "- Solar panels convert sunlight to electricity; capacity factor ~20%.\n"
     ]
    }
   ],
   "source": [
    "# Block 7 (non-interactive runner)\n",
    "import time\n",
    "\n",
    "def run_once(task, model_name=\"qwen/qwen2.5-vl-7b\"):\n",
    "    print(\"TASK USED:\\n\", task, \"\\n\")\n",
    "    t0 = time.time()\n",
    "    state = run_graph(task, model_name=model_name)\n",
    "    dt = time.time() - t0\n",
    "    print(\"\\n=== GRAPH ===\", ascii_graph())\n",
    "    print(f\"\\nâœ… Result in {dt:.2f}s:\\n{state.result}\\n\")\n",
    "    print(\"---- Evidence ----\")\n",
    "    print(\"\\n\".join(state.evidence))\n",
    "    print(\"\\n---- Scratch (last 5) ----\")\n",
    "    print(\"\\n\".join(state.scratch[-5:]))\n",
    "    return state\n",
    "\n",
    "task = (\n",
    "    \"Compare drought-tolerant gardening vs traditional gardening regarding each approach's \"\n",
    "    \"aesthetics, maintenance requirements, and inputs (fertilizer, water, etc.) for DIY home \"\n",
    "    \"landscapers redesigning Colorado front yards; compute 5*7\"\n",
    ")\n",
    "state = run_once(task)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
